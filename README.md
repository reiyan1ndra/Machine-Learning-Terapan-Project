# Machine Learning Terapan Project Report - Submission 1
*  Name: Muhammad Reiyan Indra
*  Asal: Tangerang Selatan, Banten
*  Project Theme: Predictive Analysis in Solar Power Plants
## Project Domain
This project focuses on the management and optimization of solar power plants in India. The data used in this project was collected from two solar power plants over a 34-day period. There are two types of data collected: power generation data at the inverter level and sensor data at the plant level. Each inverter has multiple lines of solar panels connected to it, while the sensor data is collected from a single array of sensors optimally placed throughout the plant. By using the available data, this project aims to provide valuable insights to enhance the efficiency and performance of solar power plants through more accurate predictions, timely maintenance, and identification of equipment issues.


![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/a5ec9fa2cbc2796fb11fdf84f3f3b9797c3cbd5e/Images/Solar%20Electricity%20Production.png)
## Business Understanding
The production of energy from solar panels is greatly influenced by weather conditions such as sunlight, clouds, and rainfall. Predicting these weather fluctuations with high accuracy, especially for a few days ahead, is challenging. Each inverter has different efficiencies and performance characteristics. Understanding how changes in efficiency affect energy production predictions is a separate challenge. Therefore, it is important to seek robust evaluation methods for predictive models to enhance accurate energy production, ensuring that the models can handle unforeseen weather variations.

### Problem Statement
From the business perspective outlined above, we aim to address: 
*  Can we achieve precise predictions of energy production from solar panels over the next couple of days?
*  How do variations in inverter efficiencies impact the overall prediction of energy production?
  
### Goals
Our objectives are:
*  Accurately forecast energy production from solar panels by incorporating inverter power values, temperature readings, and irradiation data.
*  Implement robust evaluation methods to validate the accuracy and reliability of our predictive models across varying conditions.
  
### Solutions
To achieve these goals, we will:
*  Select appropriate machine learning prediction models, considering the dataset's characteristics and objectives.

These steps are designed to enhance our ability to forecast solar panel energy production effectively, accounting for the variability in inverter efficiencies and ensuring the reliability of our predictive models under diverse environmental conditions.

## Data Understanding
The dataset used for this project can be downloaded from [Kaggle](https://www.kaggle.com/datasets/anikannal/solar-power-generation-data/data). This dataset contains 68,773 records and 9 variables. It consists of two datasets: *Plant_1_Generation_Data.csv* and *Plant_1_Weather_Sensor_Data.csv*. Each dataset contains similar variables, but with important differences: the first dataset (Plant_1_Generation_Data) includes power and yield values, while the second dataset (Plant_1_Weather_Sensor_Data) includes solar panel temperature and irradiance values. Below is a detailed explanation of each variable:
*   `DATE_TIME` : Date and time for each observation
*   `PLANT_ID`
*   `SOURCE_KEY`: Inverter ID
*   `DC_POWER`: Amount of DC power generated by the inverter (source_key) in this 15 minute interval. Units - kW.
*   `AC_POWER`: Amount of AC power generated by the inverter (source_key) in this 15 minute interval. Units - kW.
*   `DAILY_YIELD`:  a cumulative sum of power generated on that day, till that point in time.
*   `TOTAL_YIELD`:  total yield for the inverter till that point in time.
*   `AMBIENT_TEMPERATURE`: the ambient temperature at the plant.
*   `MODULE_TEMPERATURE`: There's a module (solar panel) attached to the sensor panel. This is the temperature reading for that module.
*   `IRRADIATION`: Amount of irradiation (W/m<sup>2</sup>) for the 15 minute interval.

### EDA Univariate
Here is the analysis of EDA Univariate:
*  Average of Daily Yield per Day
   ![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/15ac849066923a8ae922629bf7c702f533907a29/Images/avg%20daily%20yield.png)
*  Average of AC & DC Power Generated per Day
   ![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/15ac849066923a8ae922629bf7c702f533907a29/Images/avg%20ac%20dc%20power.png)
*  Average of Ambient and Module Temperature per Day
   ![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/15ac849066923a8ae922629bf7c702f533907a29/Images/avg%20temp.png)
*  Average of Irradiation per Day
   ![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/15ac849066923a8ae922629bf7c702f533907a29/Images/irradiation.png)

Descriptive Statistics Summary of Solar Power Generation and Weather Data:

| Statistic   | DC_POWER      | AC_POWER   | TOTAL_YIELD   | AMBIENT_TEMPERATURE | MODULE_TEMPERATURE | IRRADIATION   |
|-------------|---------------|------------|---------------|---------------------|--------------------|---------------|
| mean        | 3147.177450   | 307.778375 | 6.978728e+06  | 25.558521           | 31.244997          | 0.232305      |
| min         | 0.000000      | 0.000000   | 6.183645e+06  | 20.398505           | 18.140415          | 0.000000      |
| 25%         | 0.000000      | 0.000000   | 6.512007e+06  | 22.724491           | 21.123944          | 0.000000      |
| 50%         | 428.571429    | 41.450000  | 7.146685e+06  | 24.670178           | 24.818984          | 0.031620      |
| 75%         | 6365.468750   | 623.561161 | 7.268751e+06  | 27.960429           | 41.693659          | 0.454880      |
| max         | 14471.125000  | 1410.950000| 7.846821e+06  | 35.252486           | 65.545714          | 1.221652      |
| std         | 4036.441826   | 394.394865 | 4.162707e+05  | 3.361300            | 12.308283          | 0.301948      |

We also plot histograms to identify the frequencies of variables distribution:
*  Histogram of DC Power
  
   ![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/bdc909d55707762cc47a6b9333d78d95ad58c96f/Images/histogram.png)
   
   The histogram illustrates that DC Power values are right-skewed, with the majority concentrated in the range of 0 to 400 kW.
*  Histogram of AC Power
   ![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/bdc909d55707762cc47a6b9333d78d95ad58c96f/Images/hist%20ac%20power.png)
   
   The AC Power histogram shows a right-skewed distribution, with the highest frequency of values concentrated between 0 and 40 kW.
*  Histogram of Daily Yield
  
   ![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/bdc909d55707762cc47a6b9333d78d95ad58c96f/Images/hist%20daily%20yield.png)
   
   The daily yields range from 0 to 300 kW, showing a right-skewed distribution.
   
*  Histogram of Ambient Temperature
  
   ![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/bdc909d55707762cc47a6b9333d78d95ad58c96f/Images/hist%20ambient%20temperature.png)
   
   The ambient temperature peaked at 28 degrees Celsius, showing a moderately skewed distribution.
   
*  Histogram of Module Temperature
  
   ![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/bdc909d55707762cc47a6b9333d78d95ad58c96f/Images/hist%20module%20temperature.png)
   
   The module temperature peaked at 40 degrees Celsius, showing a moderately skewed distribution.
   
*  Histogram of Irradiation
  
   ![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/bdc909d55707762cc47a6b9333d78d95ad58c96f/Images/hist%20irradiation.png)
   
   The histogram shows that irradiation is predominantly concentrated between 0 and 0.0286 W/m<sup>2</sup> , with a left-skewed distribution.

### EDA Multivariate
Since the datasets lack categorical features, we will continue analyzing the different types of variables numerically by plotting the correlation graph and heatmap.
*  Correlation Graph:
   ![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/bdc909d55707762cc47a6b9333d78d95ad58c96f/Images/pairplot.png)
*  Heatmap:
   ![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/bdc909d55707762cc47a6b9333d78d95ad58c96f/Images/corr%20matrix.png)

   Based on the correlation matrix, it can be concluded that there is little correlation between `TOTAL_YIELD` and `AMBIENT_TEMPERATURE`, `MODULE_TEMPERATURE`, and `IRRADIATION`.

## Data Preparation

Since we do not have categorical values, we will not use one-hot encoding methods. Instead, we will continue to split the data into training and test sets, with 90% for training and 10% for testing. Additionally, we will implement standardization to transform the features into a format that is more easily processed by algorithms. Standardization is the most commonly used transformation technique in the modeling preparation stage.

### Train-test-split

Due to the large size of the dataset, the train-test-split method will allocate 90% for training and 10% for testing.

```
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)
```

Total of the overall dataset, training data, and test data are as follows:

```
Total samples in whole dataset: 36685
Total samples in train dataset: 33016
Total samples in test dataset: 3669
```

### Standardization
The next step is standardization. Standardization is the most commonly used transformation technique in the modeling preparation stage. For numerical features, we will not perform transformation using one-hot encoding like we do for categorical features. Instead, we will use the StandardScaler technique from the Scikit-learn library.

```
from sklearn.preprocessing import StandardScaler

numeric_cols = [
    'DAILY_YIELD', 'TOTAL_YIELD',
    'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION'
]

scaler = StandardScaler()
scaler.fit(X_train[numeric_cols])
X_train[numeric_cols] = scaler.transform(X_train.loc[:, numeric_cols])
X_train[numeric_cols].head()

```

StandardScaler standardizes features by subtracting the mean (average value) and then dividing by the standard deviation to shift the distribution. It produces a distribution with a standard deviation of 1 and a mean of 0. Approximately 68% of the values will fall between -1 and 1.

```
X_train[numeric_cols].describe().round(4)
```

|           | DAILY_YIELD | TOTAL_YIELD | AMBIENT_TEMPERATURE | MODULE_TEMPERATURE | IRRADIATION |
|-----------|-------------|-------------|---------------------|--------------------|-------------|
| count     | 33016.0000  | 33016.0000  | 33016.0000          | 33016.0000         | 33016.0000  |
| mean      | -0.0000     | -0.0000     | 0.0000              | 0.0000             | 0.0000      |
| std       | 1.0000      | 1.0000      | 1.0000              | 1.0000             | 1.0000      |
| min       | -1.3471     | -1.9127     | -2.2651             | -1.9431            | -1.5053     |
| 25%       | -1.0028     | -1.1181     | -0.7261             | -0.8706            | -0.9043     |
| 50%       | -0.0039     | 0.4023      | 0.0182              | 0.0570             | -0.0200     |
| 75%       | 0.8557      | 0.6952      | 0.7156              | 0.7644             | 0.7678      |
| max       | 2.0589      | 2.0854      | 2.4044              | 2.3208             | 2.7445      |

## Model Development
Here are the types of model development that might be appropriate for data prediction:
*  KNN (k-nearest neighbors)
*  Random Forest
*  Boost Regressor
*  Linear Regression
*  SVR (Support Vector Regression)

### KNN
**KNN (K-nearest Neighbors)** is a relatively simple algorithm compared to others. It uses 'feature similarity' to predict the value of new data points. In other words, each new data point is assigned a value based on how similar it is to points in the training set. KNN works by comparing the distance of a sample to other training samples and selecting the k nearest neighbors (where k is a positive integer).

```
knn = KNeighborsRegressor(n_neighbors=10)
```
Here, the parameter `n_neighbors` is set to 10, which means the algorithm will consider the 10 nearest neighbors. The parameter will then take the data from these 10 closest samples to create a new data point.

### Random Forest
**Random Forest** is an ensemble learning method used for classification, regression, and other tasks. It operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.

```
RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
```
Here, the parameter `n_estimators` is set to 50, which means it will create 50 tree branches, with a maximum depth of 16.


### Boost Regressor
A **Boost Regressor**, also known as a boosting algorithm in regression tasks, is an ensemble learning technique that combines the predictions of several base estimators (usually weak learners) to improve the overall prediction accuracy.

```
boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)
```
Here, we are using a learning_rate parameter of 0.05, which means the model will be trained with a learning rate of 0.05 and with a random_state of 55.

### Linear Regression
**Linear regression** is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.

```
lr_model = LinearRegression()
```

### SVR (Support Vector Regression)
**Support Vector Regression (SVR)** is a supervised learning algorithm that applies the principles of Support Vector Machines (SVM) to regression tasks. It finds the best-fit line within a specified margin of tolerance (epsilon) and aims to minimize the prediction error by using a subset of the training data called support vectors. The goal is to create a model that predicts continuous values while maintaining robustness against outliers.

```
svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)
```
Here, SVR is initialized with a radial basis function (RBF) kernel, C=100 for regularization strength, gamma=0.1 for kernel coefficient, and epsilon=0.1 for the margin of tolerance.

## Model Evaluation
The evaluation metric we will use for this prediction is MSE or Mean Squared Error, which calculates the average of the squared differences between actual values and predicted values. MSE is defined by the following equation:
![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/f705e93df7990701565a11e631b3826447f0b654/Images/2021071619431112f1106e20559e77c855cea11d1b1479.jpeg)

Model Performance Comparison Results:

|          | Train        | Test         |
|----------|--------------|--------------|
| KNN      | 2569.979923  | 3400.414956  |
| RF       | 639.088477   | 2514.978222  |
| Boosting | 4608.368898  | 4943.855889  |
| LinearRegression | 4123.071216 | 4448.260969 |
| SVR      | 3229.136262  | 3558.068768  |

![image.png](https://github.com/reiyan1ndra/Machine-Learning-Terapan-Project/blob/f705e93df7990701565a11e631b3826447f0b654/Images/train_test.png)

Based on the training and test scores for each model, **Random Forest (RF)** performs the best with the lowest error on both training and test sets, indicating strong performance and potential robustness in generalization. It emerges as the most promising model, demonstrating lower errors and balanced performance across training and test sets. Furthermore, it can be seen from the table that predictions using Random Forest approach the y values with differences that are quite close.

|    **     | y_true   | prediction_KNN | prediction_RF | prediction_Boosting | prediction_LinearRegression | prediction_SVR |
|---------|----------|----------------|---------------|---------------------|-----------------------------|----------------|
| 14173   | 48.6375  | 57.0           | 49.9          | 48.0                | 78.2                        | 51.2           |

This can also be measured by the R2 value, where Random Forest has a higher value compared to KNN, Boosting, Linear Regression, and SVR, indicating that Random Forest is highly effective in predicting values.
```
R2 score KNN :  0.9752660010210168
R2 score RF :  0.9817065065396017
R2 score Boosting :  0.9752660010210168
R2 score Linear Regression:  0.9676441599956099
R2 score SVR:  0.9741192559121683
```
















